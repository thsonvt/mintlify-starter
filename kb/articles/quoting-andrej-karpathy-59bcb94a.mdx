---
title: 'Quoting Andrej Karpathy'
description: 'Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $'
icon: 'newspaper'
author: 'Simon Willison'
authorId: 'simon-willison'
published: '2026-01-31'
sourceUrl: 'https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything'
topics: ["Generative AI","AI","OpenAI"]
diataxisType: 'explanation'
---

<Info>
**Original**: [Simon Willison](https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything) · 31/01/2026
</Info>

## Summary

Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K. Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K. It achieves 0.256525 CORE score, which is an ensemble metric introduced in the DCLM paper over 22 evaluations like ARC/MMLU/etc. As of the last f

## Key Insights

> "Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K."
>
> — Discussing the initial costs of training GPT-2.

> "As of the last few improvements merged into nanochat (many of them originating in modded-nanogpt repo), I can now reach a higher CORE score in 3.04 hours (~$73) on a single 8XH100 node."
>
> — Highlighting the significant cost reduction and efficiency improvements in training models.

## Topics

- [Generative AI](/kb/topics/generative-ai)
- [AI](/kb/topics/ai)
- [OpenAI](/kb/topics/openai)

---

## Full Article

```
# Quoting Andrej Karpathy
```

**Author**: Simon Willison  
**Published**: 2026-01-31  
**Source**: [https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything](https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything)

---

&lt;blockquote cite="https://twitter.com/karpathy/status/2017703360393318587">&lt;p>Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K. It achieves 0.256525 CORE score, which is an ensemble metric introduced in the DCLM paper over 22 evaluations like ARC/MMLU/etc.&lt;/p>
&lt;p>As of the last few improvements merged into nanochat (many of them originating in modded-nanogpt repo), I can now reach a higher CORE score in 3.04 hours (~$73) on a single 8XH100 node. This is a 600X cost reduction over 7 years, i.e. the cost to train GPT-2 is falling approximately 2.5X every year.&lt;/p>&lt;/blockquote>
&lt;p class="cite">&mdash; &lt;a href="https://twitter.com/karpathy/status/2017703360393318587">Andrej Karpathy&lt;/a>&lt;/p>

    &lt;p>Tags: &lt;a href="https://simonwillison.net/tags/andrej-karpathy">andrej-karpathy&lt;/a>, &lt;a href="https://simonwillison.net/tags/gpt-2">gpt-2&lt;/a>, &lt;a href="https://simonwillison.net/tags/generative-ai">generative-ai&lt;/a>, &lt;a href="https://simonwillison.net/tags/ai">ai&lt;/a>, &lt;a href="https://simonwillison.net/tags/llms">llms&lt;/a>, &lt;a href="https://simonwillison.net/tags/openai">openai&lt;/a>&lt;/p>

---

## Key Takeaways

### Notable Quotes

> Originally in 2019, GPT-2 was trained by OpenAI on 32 TPU v3 chips for 168 hours (7 days), with $8/hour/TPUv3 back then, for a total cost of approx. $43K.

*Context: Discussing the initial costs of training GPT-2.*

> As of the last few improvements merged into nanochat (many of them originating in modded-nanogpt repo), I can now reach a higher CORE score in 3.04 hours (~$73) on a single 8XH100 node.

*Context: Highlighting the significant cost reduction and efficiency improvements in training models.*

## Related Topics

- [[topics/generative-ai]]
- [[topics/ai]]
- [[topics/openai]]

---

## Related Articles

<CardGroup cols={1}>
  <Card
    title="[AINews] Anthropic's Agent Autonomy study"
    icon="newspaper"
    href="/kb/articles/ainews-anthropics-agent-autonomy-study-126b8d87"
  >
    Swyx · explanation · 67% similar
  </Card>
  <Card
    title="[AINews] OpenAI closes $110B raise from Amazon, NVIDIA, SoftBank in largest startup fundraise in history @ $840B post-money"
    icon="newspaper"
    href="/kb/articles/ainews-openai-closes-110b-raise-from-amazon-nvidia-softbank--6ca055dc"
  >
    Swyx · reference · 66% similar
  </Card>
  <Card
    title="We gotta talk about AI as a programming tool for the arts"
    icon="newspaper"
    href="/kb/articles/we-gotta-talk-about-ai-as-a-programming-tool-for-the-arts-715c2eb4"
  >
    Simon Willison · explanation · 65% similar
  </Card>
</CardGroup>

---

<Note>
Originally published at [https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything](https://simonwillison.net/2026/Jan/31/andrej-karpathy/#atom-everything).
</Note>
