---
title: 'How to do AI analysis you can actually trust'
description: 'Lenny Rachitsky discusses the challenges of AI analysis in user research, emphasizing the importance of verification to avoid misleading insights.'
icon: 'newspaper'
author: 'Lenny Rachitsky'
authorId: 'lenny-rachitsky'
published: '2026-02-17'
sourceUrl: 'https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually'
topics: ["ai-analysis","user-research","trustworthy-insights","prompting-techniques"]
diataxisType: 'how-to'
---

<Info>
**Original**: [Lenny Rachitsky](https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually) Â· 17/02/2026
</Info>

## Summary

Lenny Rachitsky discusses the challenges of AI analysis in user research, emphasizing the importance of verification to avoid misleading insights.

## Key Insights

> "The problem with AI is that the output always looks confident even when it's full of lies."
>
> â€” Introduction to the challenges of AI outputs in user research.

> "Without those checks, false but convincing-looking insights go into a deck and influence a million-dollar decision in the wrong direction."
>
> â€” Highlighting the consequences of unverified AI insights.

> "Real analysis requires sitting with the mess, noticing contradictions, weighing tangents, and catching tone shifts."
>
> â€” Describing the complexity of analyzing unstructured data.

## Topics

- [ai-analysis](/kb/topics/ai-analysis)
- [user-research](/kb/topics/user-research)
- [trustworthy-insights](/kb/topics/trustworthy-insights)
- [prompting-techniques](/kb/topics/prompting-techniques)

---

## Full Article

Hey there, Im Lenny. Each week, I answer reader questions about building product, driving growth, and accelerating your career. For more: Lennys Podcast | Lennybot | How I AI | My favorite AI/PM courses, public speaking course, and interview prep copilotSubscribe nowP.S. Get a full free year of Lovable, Manus, Replit, Gamma, n8n, Canva, ElevenLabs, Amp, Factory, Devin, Bolt, Wispr Flow, Linear, PostHog, Framer, Railway, Granola, Warp, Perplexity, Magic Patterns, Mobbin, ChatPRD, and Stripe Atlas by becoming an Insider subscriber.The problem with AI is that the output always looks confidenteven when its full of lies: made-up quotes, false insights, and completely wrong conclusions. As todays guest author, Caitlin Sullivan, puts it, These mistakes are invisible until a stakeholder asks a question you cant answer, or a decision falls apart three months later, or you realize the customer evidence behind a major investment actually had enormous holes.A user-research veteran, Caitlin has been at the bleeding edge of using AI for user research. Shes trained hundreds of product and research professionals at companies big and small on effective AI-powered customer research and advised teams at companies like Canva and YouTube. Below, she shares her four most effective techniques for getting real, trustworthy, and actionable user insights out of ChatGPT, Claude, Gemini, or your LLM of choice. Lets get into it.For more from Caitlin, find her on LinkedIn and in her new course, Claude Code for Customer Insights.P.S. You can listen to this post in convenient podcast form: Spotify / Apple / YouTube.Everyones analyzing customer data with AI. But everyones also getting answers full of slop: hallucinations, wrong conclusions, and insights that just parrot back what you already told it.Put the same customer conversation transcripts into two models and get a choose-your-own-adventure experience in return. Each model will give you a different narrative, different evidence, and wildly different product recommendations, with the same high level of confidence. Below are two real outputs from that experiment. One is misleading. One is trustworthy. Can you tell which is which?When theyre side by side, you might spot the problems with the one on the left. But thats not how this works in practice. You get one output, it reads confidently, and you build your next decision on top of it and never see whats missing. This is exactly why verification matters.Heres what separates these answers: the left output cherry-picks three enthusiastic quotes and leaps to a confident recommendation (Yes, build a screen), without questioning whether those quotes represent the full dataset. It looks persuasive, but its the AI equivalent of confirmation bias.The right output does something harder. It challenges the surface-level request (Do not interpret screen feedback as a single, literal feature request), segments users by actual need, and flags pricing risk with specific participant timestamps you can verify. Its messier and doesnt oversimplify things, but its real.The difference between the two examples above comes down to crucial steps in my workflow to address common failure modes of AI analysis. Those steps force LLMs to maintain the customers exact words, dig deep beyond superficial patterns, and catch contradictions in customer stories that will skew final recommendations. Without those checks, false but convincing-looking insights go into a deck and influence a million-dollar decision in the wrong direction.In this post, Ill show you how to get relevant and verified insights you can trust. Youll learn about four failure modes that silently break your AI-supported insights:Invented evidenceFalse or generic insightsSignal that doesnt guide better decisionsContradictory insightsIll also teach you my prompting techniques to prevent and catch these errors before they lead to the wrong final decisions. These tactics work across Claude, ChatGPT, Gemini, and with interviews, surveys, or any qualitative or mixed data youre trying to make sense of with the help of AI.Why AI struggles with customer research dataBefore we get into failure modes, you need to understand whats difficult about this kind of data for AI in the first place. Models fail with interviews and surveys in different ways.Interviews are unstructured and messy.A 45-minute research interview is a messy, wandering conversation. A participant may contradict themself. They go on tangents. They say something important at minute 8 and reframe it completely at minute 35.LLMs handle this by imposing structure and jumping to conclusions a bit too fast. They find clean themes immediately, pull quotes that fit them best, produce tidy summaries, and call it a day.But real analysis requires sitting with the mess, noticing contradictions, weighing tangents, and catching tone shifts. Without explicit guidance, AI flattens all of that into something that looks like insight but misses what actually matters.Even when surveys look structured, theyre not.Youd think a CSV would be easy to parse. Rows and columnswhats complicated about that? A lot.A column of 200 responses to Why did you cancel? is just as messy as interview data; maybe worse, because you have none of the context. In an interview, you remember that they hesitated, or had just complained about a specific feature. In a survey, you get It wasnt for me and nothing else.Your CSV may also not be as clean as you think. Different tools export differently. SurveyMonkey might put question text in headers, while Qualtrics exports headers with internal codes. Some exports even include metadata columnstimestamps, internal tagssitting right next to customer responses, without clear differentiation. If you dont tell AI which columns contain the customers voice and which to ignore, it analyzes everything as signal. Ive seen AI treat an internal note (flagged for follow-up) as something the customer said.Even structured columns hide complexity. A header that says Q3_churn probability tells AI nothing about the scale, the question wording, or whether 5/5 is good or bad.When analyzing interviews, AI models require help with structure, evidence extraction, and contradiction detection. With surveys, they require help with interpretation, column disambiguation, and understanding what sparse responses actually mean.The four failure modes below hit both data types and anything similar. Fixing these will typically 10x both the reliability and relevance of your AI analysis results.What each LLM is best used forNot all LLMs are equal for analysis work. Ive run the same analysis process across Claude, ChatGPT, and Gemini over 100 times and worked with discovery tool product teams like Maze testing prompts across models to see what delivers.Heres what you need to know about each model:Claude: Best for thorough analysis with depth and nuance. Delivers more quotes and covers more ground with less pushing. The tradeoff: it gives you the whole brain dump, so themes arent always provenyou get breadth, not just the safe patterns.Gemini (and NotebookLM): Best for highly evidenced themes and now video analysis. Gives you fewer themes but with stronger grounding. Expect to prompt multiple times to get completeness, and to ask for longer quotes. Unique advantage: it can analyze non-verbal behaviors in video, which the others cant (yet).ChatGPT: Best for final framing and stakeholder communication. Most creative of the threeincluding with verbatim quotes, unfortunately. Least reliable for real evidence (combines quotes), but excels at packaging relevant findings for a specific audience.Let me show you what I mean:Want to see the transcript used in this example? Click here.Unless I give these models more instruction, there are meaningful differences in output. Theres theme overlap, but ChatGPT misses the users value/price sensitivity reaction, and all three models give different confidence scores and quotessome verbatim, some summarized.This becomes obvious when we can see all three side by side, but most teams have one LLM enterprise account and wont see the shortcomings of the one they use. ChatGPT summarizes and mashes together verbatim quotes, Claude is more conservative with confidence scores, and Gemini often chooses too-short snippets of customer voice.My recommendation: If you have a choice, use Claude for analysis work. It covers more ground while staying rooted in the actual data. You get depth and breadth without as much pushing. The tradeoff is that it doesnt filter for you. Youll often get validated patterns and half-formed hypotheses presented on equal ground, and youll need to verify that themes are well-evidenced. But thats a better starting point than having to prompt three times before being sure your analysis partner hasnt missed something.A note on examples: For consistency, the examples throughout this post typically use ChatGPT. Its still the most widely used model among my client teams and students, and its also the most prone to the specific failure modes Im covering. The fixes work and improve results across all three models.Four ways AI data analyses lie to you, and how to fix themAfter more than 2,000 hours of testing customer discovery workflows with AI, Ive found that there are four distinct failure modes for AI analysisand reliable fixes for each one that consistently work across platforms, data types, models, and workflows.Failure mode #1: Invented evidenceWhat the problem looks likeDespite massive improvements across most reasoning models, hallucinations are still abundant. When I look over the shoulders of product people running analysis, I see two hallucination types all the time:Completely fictionalized quotes (still happens among all three major LLMs)Frankenstein quotes sewn together from multiple sources that somewhat represent what the user was saying . . . but isnt actually their words (particularly common in ChatGPT)Both types go unnoticed unless youre checking every quote manually, but both are often caused by the way you prompt. You can pretty easily and accidentally trigger ChatGPT to combine multiple customer quotes in ways that can harm our understanding of what the customer was saying. When you add phrases like max. 100 words or for each theme, give a punchy and representative quote that captures it (12 words), youll almost always get mash-ups. Highlights in the example below are combinations like this, not the customers exact real words.Why this happensLLMs dont retrieve quotes like a search engine; they generate text thats statistically likely given the context. Generation and retrieval are fundamentally different. The model predicts what a quote should look like. If the context is about phone-checking frustration, it generates plausible phone-checking-frustration language. Sometimes that matches the original, sometimes its a near-miss, and sometimes its fabricated.Verbatim is also an ambiguous word to prompt a model with. Exact characters? Can punctuation differ? What about filler words? Where does the quote start and end? The model fills these gaps with assumptions you never see. Even participant IDs and timestamps can be faked. A citation like [P03, 14:30] looks authoritative but means nothing if the quote is invented.The fix: Quote selection rules + verificationThe solution to this problem, no matter your model, data type, or workflow, has two parts. First, define what a valid quote actually looks likeyour quote ruleswhich removes the ambiguity that lets AI fill in gaps. And then verify that quotes in the resulting AI analysis actually exist before you allow the model to use them.1. Define your quote rulesAdd this to your analysis prompt:QUOTE SELECTION RULESStart where the thought begins, and continue until fully expressedInclude reasoning, not just conclusionsKeep hedges and qualifiers they signal uncertaintyInclude emotional language when presentCite with participant ID and approximate timestamp [P02 ~14:30]Do not combine statements from different parts of the interviewIf a quote would exceed 3 sentences, break it into separate quotesThis removes ambiguity. The model now knows what verbatim means to you: where to start, where to stop, what to include, what not to combine.I always encourage client teams and course participants to think critically about what makes a quote good to them. Youll likely get much better results right away with my prompt snippet (its my favorite copy-paste inclusion) but even better results if you add your own definitions.2. Verify before you useAfter your initial analysis, use this verification prompt to have the LLM confirm that these are real quotes:QUOTE VERIFICATIONFor each quote in the analysis above:Confirm the quote exists verbatim in the source transcriptIf the quote is a close paraphrase but not exact, flag it and provide the actual wordingIf the quote cannot be located, mark as NOT FOUNDOutput format:Quote: [the quote]Status: VERIFIED / PARAPHRASE / NOT FOUNDIf paraphrase: Actual wording: [what they said]Location: [Participant ID, timestamp, or line number]Heres what happens when you run that:A majority of the quotes in the previous ChatGPT output were paraphrased, not original verbatim customer statements. This happened with a request for a small set of quotes, so imagine what happens when youre digging through 20 interviews and getting just as many patterns back.Without verification, quotes like these end up in your deck attributed to a real participant. Sometimes its no big deal. Other times, its the difference between product language that strongly resonates and messaging that doesnt convert.This often takes just an extra five minutes, depending on how much data youre dealing with. But it catches errors that would otherwise undermine the evidence behind your product decisions.Failure mode #2: False or generic insightsWhat the problem looks likeAI finds themes that are too broad and generic to act on, or biased by what you accidentally primed it with. In interviews, you get themes that could describe any product in your category. I hear this constantly from PMs: The AI analysis just told me what I already know or These insights are too generic. I cant do anything with them.They get outputs like:Price is a factor in decisionsPeople value reliabilityUsers want more real-time informationTrue, probably, but useless for tough decisionswe need to get deeper than that. These themes could come from so many studies out there. Since Im working with my fake Whoop data here, they could also easily come from any wearables study.Themes like these dont tell you whether your users want this new feature youre exploring enough to justify the investment, or whether adding it would alienate the customers who chose you specifically because youre different.Why this happensAI defaults to finding consensus, because LLMs are pattern-finding machines. They surface the (obvious) patterns that easily rise to the top, finding what multiple participants mentioned, and then they generate a pattern-matched theme.The truly most important insight might be something only a few people said in this particular batch of interviews but that, if shared by more customers, would be a noteworthy business signal. Or the most important insight might even be the tension between what people say they want and what their behavior suggests.LLMs also bring priors from training. If the model has seen thousands of churn analyses where price is the #1 theme, it will weight toward price even if your full dataset doesnt support it.In surveys, that tendency to superficially pattern-match is even worse. When someone writes Its not for me when cancelling, AI has to guess what that means. Without guidance, itll likely lump that response with others into a generic value perception theme. But not worth it could mean:Too expensive for what Id getToo data-intensive and Im not a serious enough athleteI dont want another device to chargeI need a screen and Whoop doesnt have oneIts one response with four completely different implications for your product decision. Multiply that ambiguity across hundreds of survey responses, and your themes become meaningless averages that dont make decisions any easier.LLMs are trained to find consensus and compress information. Specificity and valuable edge cases get lost. And if your prompt mentions pricing issues, watch how many responses suddenly get coded as pricing-related.In some cases, that can be helpful, because the model makes sure all outputs are relevant to the specific thing youre working on. But in many cases, it can be biased cherry-picking from the start.Heres an example:These could describe any wearables study. We cant make a hardware decision from this.Another example:I asked ChatGPT to find me theme clusters and counts for the churn survey question What did you hope to do that Whoop failed to support you with?The results below are clusters that dont help us make this decision either. So 18% of churned respondents need more actionable guidance, which sounds like a job-to-be-done. But there are too many possible directions within that cluster for us to make a decision more easily. Should Whoop focus on clearer metrics or workout plans, or both? Plus, most of this has nothing to do with our screen decision.When we ask AI to cluster survey responses, we need to give it clear direction with context, or were leaving room for mediocre decisions and more manual work.The fix: Context loading that actually guides interpretationMost people are used to prompt frameworks that have sections like Role, Context, Task, Format, and so on. Context to most of us means including a few lines of background information in the prompt, somewhere near the beginning. When were using AI for analysis, that often focuses on the point of this current customer discovery. Think: objectives, hypotheses, and what part of the product were working on.In the past year, Ive seen more and more people turn the prompts context section into four paragraphs of anything they could think of about their work, often dictated in a stream of thought while eating lunch.But neither three lines of objectives nor the whole unstructured backstory is good enough. Effective context loading has at least four components that shape how AI interprets everything that follows:Project context tells AI the scope and stakes. Exploring whether to add a screen is a specific decision with constraints. Doing customer research is vague, so AI defaults to generic analysis because you gave it no frame.Business goal tells AI what youre trying to achieve. If you need to know whether a feature would attract new users vs. alienate existing ones in order to prioritize building it, say that. AI will weight evidence toward answering your question and addressing your decision, not the decision it assumes youre making.Product context gives AI domain knowledge. Without it, AI interprets I want to see my data generically. With it, AI understands that statement in the context of a screenless wearable competing against Apple Watcha completely different interpretation.Participant overview tells AI whos speaking. I need real-time data from a churned Garmin switcher means something different than the same words from a loyal user whos never tried a competitor. AI can only weight evidence correctly if it knows who the evidence is coming from.The good news is that a lot of what I see people add to the context in their prompts is superfluous. You often dont need as much information as you think, but it needs to be clear, direct, and relevant information, like the four items above.For interviews, put this context into an analysis prompt (or use as a single prompt): Read more

---

## Related Articles

<CardGroup cols={1}>
  <Card
    title="How to do AI analysis you can actually trust"
    icon="newspaper"
    href="/kb/articles/how-to-do-ai-analysis-you-can-actually-trust-52ad310d"
  >
    Lenny Rachitsky Â· how-to Â· 86% similar
  </Card>
  <Card
    title="ðŸŽ™ï¸ This week on How I AI: How to build your own AI developer tools with Claude Code"
    icon="newspaper"
    href="/kb/articles/-this-week-on-how-i-ai-how-to-build-your-own-ai-developer-to-db8b7ca4"
  >
    Lenny Rachitsky Â· how-to Â· 67% similar
  </Card>
  <Card
    title="Building AI product sense, part 2"
    icon="newspaper"
    href="/kb/articles/building-ai-product-sense-part-2-0a3d6458"
  >
    Lenny Rachitsky Â· explanation Â· 66% similar
  </Card>
</CardGroup>

---

<Note>
Originally published at [https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually](https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually).
</Note>
