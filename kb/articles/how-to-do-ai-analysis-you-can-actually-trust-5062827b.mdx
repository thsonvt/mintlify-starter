---
title: 'How to do AI analysis you can actually trust'
description: 'The problem with AI is that the output always looks confidentâ€”even when itâ€™s full of lies. *ğŸ‘‹ Hey there, Iâ€™m Lenny. Each week, I answer reader questi'
icon: 'newspaper'
author: 'Lenny Rachitsky'
authorId: 'lenny-rachitsky'
published: '2026-02-17'
sourceUrl: 'https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually'
topics: ["Prompt Engineering","AI Agents","Customer Research"]
diataxisType: 'how-to'
---

<Info>
**Original**: [Lenny Rachitsky](https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually) Â· 17/02/2026
</Info>

## Summary

The problem with AI is that the output always looks confidentâ€”even when itâ€™s full of lies. *ğŸ‘‹ Hey there, Iâ€™m Lenny. Each week, I answer reader questions about building product, driving growth, and accelerating your career. For more: [Lennyâ€™s Podcast](https://www.lennysnewsletter.com/podcast) | [Lennybot](https://www.lennybot.com/) | [How I AI](https://www.youtube.com/@howiaipodcast) | My

## Key Insights

> "The problem with AI is that the output always looks confidentâ€”even when itâ€™s full of lies."
>
> â€” Highlighting the issue of misleading confidence in AI-generated insights.

> "These mistakes are invisible until a stakeholder asks a question you canâ€™t answer."
>
> â€” Discussing the invisible risks of unverified AI analysis in decision-making.

> "Without those checks, false but convincing-looking insights go into a deck and influence a million-dollar decision in the wrong direction."
>
> â€” Emphasizing the importance of verification steps in AI analysis to prevent costly mistakes.

## Topics

- [Prompt Engineering](/kb/topics/prompt-engineering)
- [AI Agents](/kb/topics/ai-agents)
- [Customer Research](/kb/topics/customer-research)

---

## Full Article

*ğŸ‘‹ Hey there, Iâ€™m Lenny. Each week, I answer reader questions about building product, driving growth, and accelerating your career. For more: [Lennyâ€™s Podcast](https://www.lennysnewsletter.com/podcast) | [Lennybot](https://www.lennybot.com/) | [How I AI](https://www.youtube.com/@howiaipodcast) | My favorite [AI/PM courses](https://maven.com/lenny), [public speaking course](https://ultraspeaking.com/lennyslist?via=lenny), and [interview prep copilot](https://www.benerez.com/copilot/lenny)*

```
[Subscribe now](https://www.lennysnewsletter.com/subscribe?)
```

*P.S. Get a full free year of Lovable, Manus, Replit, Gamma, n8n, Canva, ElevenLabs, Amp, Factory, Devin, Bolt, Wispr Flow, Linear, PostHog, Framer, Railway, Granola, Warp, Perplexity, Magic Patterns, Mobbin, ChatPRD, and Stripe Atlas [by becoming an Insider subscriber](http://Subscribers get a free year of).*

---

The problem with AI is that the output always looks confidentâ€”even when itâ€™s full of lies: made-up quotes, false insights, and completely wrong conclusions. As todayâ€™s guest author, [Caitlin Sullivan](https://www.linkedin.com/in/caitlindsullivan/), puts it, â€œ**These mistakes are invisible until a stakeholder asks a question you canâ€™t answer, or a decision falls apart three months later, or you realize the â€˜customer evidenceâ€™ behind a major investment actually had enormous holes.â€**

A user-research veteran, Caitlin has been at the bleeding edge of using AI for user research. Sheâ€™s trained hundreds of product and research professionals at companies big and small on effective AI-powered customer research and advised teams at companies like Canva and YouTube. **Below, she shares her four most effective techniques for getting real, trustworthy, and actionable user insights out of ChatGPT, Claude, Gemini, or your LLM of choice.** Letâ€™s get into it.

*For more from Caitlin, find her on [LinkedIn](https://www.linkedin.com/in/caitlindsullivan/) and in her new course, [Claude Code for Customer Insights](https://bit.ly/3ZB1jd8).*

*P.S. You can listen to this post in convenient podcast form: [Spotify](https://open.spotify.com/show/0IIunA06qMtrcQLfypTooj) / [Apple](https://podcasts.apple.com/us/podcast/lennys-reads/id1810314693) / [YouTube](https://www.youtube.com/@lennysreads).*

---

```
[![](https://substackcdn.com/image/fetch/$s_!o_Ba!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd6012c-6a31-4cc1-962e-35820925743f_1456x970.png)](https://substackcdn.com/image/fetch/$s_!o_Ba!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd6012c-6a31-4cc1-962e-35820925743f_1456x970.png)
```

Everyoneâ€™s â€œanalyzingâ€ customer data with AI. But everyoneâ€™s also getting answers full of slop: hallucinations, wrong conclusions, and â€œinsightsâ€ that just parrot back what you already told it.

Put the same customer conversation transcripts into two models and get a choose-your-own-adventure experience in return. Each model will give you a different narrative, different â€œevidence,â€ and wildly different product recommendations, with the same high level of confidence. Below are two real outputs from that experiment. One is misleading. One is trustworthy. Can you tell which is which?

```
[![](https://substackcdn.com/image/fetch/$s_!xmwL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe002abc-bf87-4b23-9908-7ffd44f62703_1456x550.png)](https://substackcdn.com/image/fetch/$s_!xmwL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe002abc-bf87-4b23-9908-7ffd44f62703_1456x550.png)
```

When theyâ€™re side by side, you might spot the problems with the one on the left. But thatâ€™s not how this works in practice. You get one output, it reads confidently, and you build your next decision on top of it and never see whatâ€™s missing. This is exactly why verification matters.

Hereâ€™s what separates these answers: the left output cherry-picks three enthusiastic quotes and leaps to a confident recommendation (â€œYes, build a screenâ€), without questioning whether those quotes represent the full dataset. It looks persuasive, but itâ€™s the AI equivalent of confirmation bias.

The right output does something harder. It challenges the surface-level request (â€œDo not interpret screen feedback as a single, literal feature requestâ€), segments users by actual need, and flags pricing risk with specific participant timestamps you can verify. Itâ€™s messier and doesnâ€™t oversimplify things, but itâ€™s real.

The difference between the two examples above comes down to crucial steps in my workflow to address common failure modes of AI analysis. Those steps force LLMs to maintain the customerâ€™s exact words, dig deep beyond superficial patterns, and catch contradictions in customer stories that will skew final recommendations. Without those checks, false but convincing-looking insights go into a deck and influence a million-dollar decision in the *wrong* direction.

In this post, Iâ€™ll show you how to get relevant and verified insights you can trust. Youâ€™ll learn about four failure modes that silently break your AI-supported insights:

1. Invented evidence
2. False or generic insights
3. â€œSignalâ€ that doesnâ€™t guide better decisions
4. Contradictory insights

Iâ€™ll also teach you my prompting techniques to prevent and catch these errors before they lead to the wrong final decisions. These tactics work across Claude, ChatGPT, Gemini, and with interviews, surveys, or any qualitative or mixed data youâ€™re trying to make sense of with the help of AI.

### **Why AI struggles with customer research data**

Before we get into failure modes, you need to understand whatâ€™s difficult about this kind of data for AI in the first place. Models fail with interviews and surveys in different ways.

**Interviews are unstructured and messy.**

A 45-minute research interview is a messy, wandering conversation. A participant may contradict themself. They go on tangents. They say something important at minute 8 and reframe it completely at minute 35.

LLMs handle this by imposing structure and jumping to conclusions a bit too fast. They find clean themes immediately, pull quotes that fit them best, produce tidy summaries, and call it a day.

But real analysis requires sitting with the mess, noticing contradictions, weighing tangents, and catching tone shifts. Without explicit guidance, AI flattens all of that into something that *looks* like insight but misses what actually matters.

**Even when surveys look structured, theyâ€™re not.**

Youâ€™d think a CSV would be easy to parse. Rows and columnsâ€”whatâ€™s complicated about that? A lot.

A column of 200 responses to â€œWhy did you cancel?â€ is just as messy as interview data; maybe worse, because you have none of the context. In an interview, you remember that they hesitated, or had just complained about a specific feature. In a survey, you get â€œIt wasnâ€™t for meâ€ and nothing else.

Your CSV may also not be as clean as you think. Different tools export differently. SurveyMonkey might put question text in headers, while Qualtrics exports headers with internal codes. Some exports even include metadata columnsâ€”timestamps, internal tagsâ€”sitting right next to customer responses, without clear differentiation. If you donâ€™t tell AI which columns contain the customerâ€™s voice and which to ignore, it analyzes everything as signal. Iâ€™ve seen AI treat an internal note (â€œflagged for follow-upâ€) as something the customer said.

Even â€œstructuredâ€ columns hide complexity. A header that says â€œQ3\_churn probabilityâ€ tells AI nothing about the scale, the question wording, or whether 5/5 is good or bad.

When analyzing interviews, AI models require help with structure, evidence extraction, and contradiction detection. With surveys, they require help with interpretation, column disambiguation, and understanding what sparse responses actually mean.

The four failure modes below hit both data types and anything similar. Fixing these will typically 10x both the reliability and relevance of your AI analysis results.

## **What each LLM is best used for**

Not all LLMs are equal for analysis work. Iâ€™ve run the same analysis process across Claude, ChatGPT, and Gemini over 100 times and worked with discovery tool product teams like Maze testing prompts across models to see what delivers.

Hereâ€™s what you need to know about each model:

**Claude:** Best for thorough analysis with depth and nuance. Delivers more quotes and covers more ground with less pushing. The tradeoff: it gives you the whole brain dump, so themes arenâ€™t always â€œprovenâ€â€”you get breadth, not just the safe patterns.

**Gemini (and NotebookLM):** Best for highly evidenced themes and now video analysis. Gives you fewer themes but with stronger grounding. Expect to prompt multiple times to get completeness, and to ask for longer quotes. Unique advantage: it can analyze non-verbal behaviors in video, which the others canâ€™t (yet).

**ChatGPT:** Best for final framing and stakeholder communication. Most creative of the threeâ€”including with â€œverbatim quotes,â€ unfortunately. Least reliable for real evidence (combines quotes), but excels at packaging relevant findings for a specific audience.

Let me show you what I mean:

```
[![Want to see the transcript used in this example?Click here.](https://substackcdn.com/image/fetch/$s_!TDI0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12cb6ecf-5611-47cf-96f6-98a48b704443_1456x502.png)](https://substackcdn.com/image/fetch/$s_!TDI0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12cb6ecf-5611-47cf-96f6-98a48b704443_1456x502.png)
```

Want to see the transcript used in this example? [Click here](https://www.dropbox.com/scl/fi/afc3lm05x9eaxi81fgt7o/WHOOP-Test-Data-Screen-Exploration-Q42025-P01.txt?rlkey=x08j470e3cfv84uocn6ahwroy&st=2eirjv0i&dl=0).

Unless I give these models more instruction, there are meaningful differences in output. Thereâ€™s theme overlap, but ChatGPT misses the userâ€™s value/price sensitivity reaction, and all three models give different confidence scores and quotesâ€”some verbatim, some summarized.

This becomes obvious when we can see all three side by side, but most teams have one LLM enterprise account and wonâ€™t see the shortcomings of the one they use. ChatGPT summarizes and mashes together verbatim quotes, Claude is more conservative with confidence scores, and Gemini often chooses too-short snippets of customer voice.

**My recommendation:** If you have a choice, use Claude for analysis work. It covers more ground while staying rooted in the actual data. You get depth and breadth without as much pushing. The tradeoff is that it doesnâ€™t filter for you. Youâ€™ll often get validated patterns and half-formed hypotheses presented on equal ground, and youâ€™ll need to verify that themes are well-evidenced. But thatâ€™s a better starting point than having to prompt three times before being sure your analysis partner hasnâ€™t missed something.

**A note on examples:** For consistency, the examples throughout this post typically use ChatGPT. Itâ€™s still the most widely used model among my client teams and students, and itâ€™s also the most prone to the specific failure modes Iâ€™m covering. The fixes work and improve results across all three models.

## **Four ways AI data analyses lie to you, and how to fix them**

After more than 2,000 hours of testing customer discovery workflows with AI, Iâ€™ve found that there are four distinct failure modes for AI analysisâ€”and reliable fixes for each one that consistently work across platforms, data types, models, and workflows.

### **Failure mode #1: Invented evidence**

#### **What the problem looks like**

Despite massive improvements across most reasoning models, hallucinations are still abundant. When I look over the shoulders of product people running analysis, I see two hallucination types all the time:

```
* Completely fictionalized quotes (still happens among all three major LLMs)
* Frankenstein quotes sewn together from multiple sources that somewhat represent what the user was saying . . . but isnâ€™t actually their words (particularly common in ChatGPT)
```

Both types go unnoticed unless youâ€™re checking every quote manually, but both are often caused by *the way you prompt*. You can pretty easily and accidentally trigger ChatGPT to combine multiple customer quotes in ways that can harm our understanding of what the customer was saying. When you add phrases like â€œmax. 100 wordsâ€ or â€œfor each theme, give a punchy and representative quote that captures it (â‰¤12 words),â€ youâ€™ll almost always get mash-ups. Highlights in the example below are combinations like this, not the customerâ€™s exact real words.

```
[![](https://substackcdn.com/image/fetch/$s_!z79y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb4c6aa2-2fd6-40bf-b0ea-7fd5a7e0ab64_1600x770.png)](https://substackcdn.com/image/fetch/$s_!z79y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb4c6aa2-2fd6-40bf-b0ea-7fd5a7e0ab64_1600x770.png)
```

#### **Why this happens**

LLMs donâ€™t retrieve quotes like a search engine; they generate text thatâ€™s statistically likely given the context. Generation and retrieval are fundamentally different. The model predicts what a quote should look like. If the context is about phone-checking frustration, it generates plausible phone-checking-frustration language. Sometimes that matches the original, sometimes itâ€™s a near-miss, and sometimes itâ€™s fabricated.

â€œVerbatimâ€ is also an ambiguous word to prompt a model with. Exact characters? Can punctuation differ? What about filler words? Where does the quote start and end? The model fills these gaps with assumptions you never see. Even participant IDs and timestamps can be faked. A citation like â€œ[P03, 14:30]â€ looks authoritative but means nothing if the quote is invented.

#### **The fix: Quote selection rules + verification**

The solution to this problem, no matter your model, data type, or workflow, has two parts. First, define what a valid quote actually looks likeâ€”your quote â€œrulesâ€â€”which removes the ambiguity that lets AI fill in gaps. And then verify that quotes in the resulting AI analysis actually exist before you allow the model to use them.

**1. Define your quote â€œrulesâ€**

Add this to your analysis prompt:

> `QUOTE SELECTION RULES`
>
> * `Start where the thought begins, and continue until fully expressed`
> * `Include reasoning, not just conclusions`
> * `Keep hedges and qualifiers â€” they signal uncertainty`
> * `Include emotional language when present`
> * `Cite with participant ID and approximate timestamp [P02 ~14:30]`
> * `Do not combine statements from different parts of the interview`
> * `If a quote would exceed 3 sentences, break it into separate quotes`

This removes ambiguity. The model now knows what â€œverbatimâ€ means *to you*: where to start, where to stop, what to include, what not to combine.

I always encourage client teams and course participants to think critically about what makes a quote â€œgoodâ€ to them. Youâ€™ll likely get much better results right away with my prompt snippet (itâ€™s my favorite copy-paste inclusion) but even better results if you add your own definitions.

**2. Verify before you use**

After your initial analysis, use this verification prompt to have the LLM confirm that these are real quotes:

> `QUOTE VERIFICATION`
>
> `For each quote in the analysis above:`
>
> 1. `Confirm the quote exists verbatim in the source transcript`
> 2. `If the quote is a close paraphrase but not exact, flag it and provide the actual wording`
> 3. `If the quote cannot be located, mark as NOT FOUND`
>
> `Output format:`
>
> * `Quote: [the quote]`
> * `Status: VERIFIED / PARAPHRASE / NOT FOUND`
> * `If paraphrase: Actual wording: [what they said]`
> * `Location: [Participant ID, timestamp, or line number]`

Hereâ€™s what happens when you run that:

```
[![](https://substackcdn.com/image/fetch/$s_!J6mn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F949bc484-15cc-4ea6-bb72-82451b0271d7_1600x1266.png)](https://substackcdn.com/image/fetch/$s_!J6mn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F949bc484-15cc-4ea6-bb72-82451b0271d7_1600x1266.png)
```

A majority of the quotes in the previous ChatGPT output were paraphrased, not original verbatim customer statements. This happened with a request for a small set of quotes, so imagine what happens when youâ€™re digging through 20 interviews and getting just as many patterns back.

Without verification, â€œquotesâ€ like these end up in your deck attributed to a real participant. Sometimes itâ€™s no big deal. Other times, itâ€™s the difference between product language that strongly resonates and messaging that doesnâ€™t convert.

This often takes just an extra five minutes, depending on how much data youâ€™re dealing with. But it catches errors that would otherwise undermine the evidence behind your product decisions.

### **Failure mode #2: False or generic insights**

#### **What the problem looks like**

AI finds themes that are too broad and generic to act on, or biased by what you accidentally primed it with. In interviews, you get themes that could describe any product in your category. I hear this constantly from PMs: â€œThe AI analysis just told me what I already knowâ€ or â€œThese insights are too generic. I canâ€™t do anything with them.â€

They get outputs like:

```
* â€œPrice is a factor in decisionsâ€
* â€œPeople value reliabilityâ€
* â€œUsers want more real-time informationâ€
```

True, probably, but useless for tough decisionsâ€”we need to get deeper than that. These themes could come from so many studies out there. Since Iâ€™m working with my fake Whoop data here, they could also easily come from any wearables study.

Themes like these donâ€™t tell you whether *your* users want *this* new feature youâ€™re exploring enough to justify the investment, or whether adding it would alienate the customers who chose you specifically because youâ€™re different.

#### **Why this happens**

AI defaults to finding *consensus*, because LLMs are pattern-finding machines. They surface the (obvious) patterns that easily rise to the top, finding what multiple participants mentioned, and then they generate a pattern-matched theme.

The truly most important insight might be something only a few people said in this particular batch of interviews but that, if shared by more customers, would be a noteworthy business signal. Or the most important insight might even be the tension between what people say they want and what their behavior suggests.

LLMs also bring priors from training. If the model has seen thousands of churn analyses where price is the #1 theme, it will weight toward price even if your full dataset doesnâ€™t support it.

In surveys, that tendency to superficially pattern-match is even worse. When someone writes â€œItâ€™s not for meâ€ when cancelling, AI has to guess what that means. Without guidance, itâ€™ll likely lump that response with others into a generic â€œvalue perceptionâ€ theme. But â€œnot worth itâ€ could mean:

```
* Too expensive for what Iâ€™d get
* Too data-intensive and Iâ€™m not a serious enough athlete
* I donâ€™t want another device to charge
* I need a screen and Whoop doesnâ€™t have one
```

Itâ€™s one response with four completely different implications for your product decision. Multiply that ambiguity across hundreds of survey responses, and your â€œthemesâ€ become meaningless averages that donâ€™t make decisions any easier.

LLMs are trained to find consensus and compress information. Specificity and valuable edge cases get lost. And if your prompt mentions â€œpricing issues,â€ watch how many responses suddenly get coded as pricing-related.

In some cases, that can be helpful, because the model makes sure all outputs are relevant to the specific thing youâ€™re working on. But in many cases, it can be biased cherry-picking from the start.

**Hereâ€™s an example:**

```
[![](https://substackcdn.com/image/fetch/$s_!xss5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019c47ac-f471-4caf-b9bc-7ba83bbe9587_1600x589.png)](https://substackcdn.com/image/fetch/$s_!xss5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019c47ac-f471-4caf-b9bc-7ba83bbe9587_1600x589.png)
```

These could describe any wearables study. We canâ€™t make a hardware decision from this.

**Another example:**

I asked ChatGPT to find me theme clusters and counts for the churn survey question â€œWhat did you hope to do that Whoop failed to support you with?â€

The results below are clusters that donâ€™t help us make this decision either. So 18% of churned respondents need â€œmore actionable guidance,â€ which sounds like a job-to-be-done. But there are too many possible directions within that cluster for us to make a decision more easily. Should Whoop focus on clearer metrics or workout plans, or both? Plus, most of this has nothing to do with our screen decision.

```
[![](https://substackcdn.com/image/fetch/$s_!dtDi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4259eb73-5315-4845-97f2-d6a16c53e6b7_1600x940.png)](https://substackcdn.com/image/fetch/$s_!dtDi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4259eb73-5315-4845-97f2-d6a16c53e6b7_1600x940.png)
```

When we ask AI to cluster survey responses, we need to give it clear direction with context, or weâ€™re leaving room for mediocre decisions and more manual work.

#### **The fix: Context loading that actually guides interpretation**

Most people are used to prompt frameworks that have sections like Role, Context, Task, Format, and so on. Context to most of us means including a few lines of background information in the prompt, somewhere near the beginning. When weâ€™re using AI for analysis, that often focuses on the point of this current customer discovery. Think: objectives, hypotheses, and what part of the product weâ€™re working on.

In the past year, Iâ€™ve seen more and more people turn the promptâ€™s â€œcontextâ€ section into four paragraphs of anything they could think of about their work, often dictated in a stream of thought while eating lunch.

But neither three lines of objectives nor the whole unstructured backstory is good enough. Effective context loading has *at least* four components that shape how AI interprets everything that follows:

1. **Project context** tells AI the scope and stakes. â€œExploring whether to add a screenâ€ is a specific decision with constraints. â€œDoing customer researchâ€ is vague, so AI defaults to generic analysis because you gave it no frame.
2. **Business goal** tells AI what youâ€™re trying to achieve. If you need to know whether a feature would attract new users vs. alienate existing ones in order to prioritize building it, say that. AI will weight evidence toward answering *your* question and addressing *your* decision, not the decision it assumes youâ€™re making.
3. **Product context** gives AI domain knowledge. Without it, AI interprets â€œI want to see my dataâ€ generically. With it, AI understands that statement in the context of a screenless wearable competing against Apple Watchâ€”a completely different interpretation.
4. **Participant overview** tells AI whoâ€™s speaking. â€œI need real-time dataâ€ from a churned Garmin switcher means something different than the same words from a loyal user whoâ€™s never tried a competitor. AI can only weight evidence correctly if it knows who the evidence is coming from.

The good news is that a lot of what I see people add to the context in their prompts is superfluous. You often donâ€™t need as much information as you think, but it needs to be clear, direct, and relevant information, like the four items above.

**For interviews, put this context into an analysis prompt (or use as a single prompt):**

```
[Read more](https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually)
```

---

## Related Articles

<CardGroup cols={1}>
  <Card
    title="Building AI product sense, part 2"
    icon="newspaper"
    href="/kb/articles/building-ai-product-sense-part-2-0a3d6458"
  >
    Lenny Rachitsky Â· tutorial Â· 78% similar
  </Card>
  <Card
    title="How to do AI analysis you can actually trust"
    icon="newspaper"
    href="/kb/articles/how-to-do-ai-analysis-you-can-actually-trust-52ad310d"
  >
    Lenny Rachitsky Â· how-to Â· 77% similar
  </Card>
  <Card
    title="Experts Have World Models. LLMs Have Word Models."
    icon="newspaper"
    href="/kb/articles/experts-have-world-models-llms-have-word-models-78eca57f"
  >
    Swyx Â· explanation Â· 71% similar
  </Card>
</CardGroup>

---

<Note>
Originally published at [https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually](https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually).
</Note>
