---
title: 'GLM-5: From Vibe Coding to Agentic Engineering'
description: 'This is a huge new MIT-licensed model: 754B parameters and 1.51TB on Hugging Face This is a *huge* new MIT-licensed model: 754B parameters and [1.51TB'
icon: 'newspaper'
author: 'Simon Willison'
authorId: 'simon-willison'
published: '2026-02-11'
sourceUrl: 'https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything'
topics: ["Agent-Native Architecture","Prompt Engineering","AI Agents"]
diataxisType: 'explanation'
---

<Info>
**Original**: [Simon Willison](https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything) · 11/02/2026
</Info>

## Summary

This is a huge new MIT-licensed model: 754B parameters and 1.51TB on Hugging Face This is a *huge* new MIT-licensed model: 754B parameters and [1.51TB on Hugging Face](https://huggingface.co/zai-org/GLM-5) twice the size of [GLM-4.7](https://huggingface.co/zai-org/GLM-4.7) which was 368B and 717GB (4.5 and 4.6 were around that size too).

## Key Insights

> "This is a huge new MIT-licensed model: 754B parameters and 1.51TB on Hugging Face"
>
> — Discussing the size and significance of the GLM-5 model.

> "It's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs"
>
> — Commenting on the emerging term 'Agentic Engineering' for software development with LLMs.

> "I ran my 'Generate an SVG of a pelican riding a bicycle' prompt through GLM-5"
>
> — Illustrating the practical use of GLM-5 with a creative example.

## Topics

- [Agent-Native Architecture](/kb/topics/agent-native-architecture)
- [Prompt Engineering](/kb/topics/prompt-engineering)
- [AI Agents](/kb/topics/ai-agents)

---

## Full Article

```
# GLM-5: From Vibe Coding to Agentic Engineering
```

**Author**: Simon Willison  
**Published**: 2026-02-11  
**Source**: [https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything](https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything)

---

**[GLM-5: From Vibe Coding to Agentic Engineering](https://z.ai/blog/glm-5)**

This is a *huge* new MIT-licensed model: 754B parameters and [1.51TB on Hugging Face](https://huggingface.co/zai-org/GLM-5) twice the size of [GLM-4.7](https://huggingface.co/zai-org/GLM-4.7) which was 368B and 717GB (4.5 and 4.6 were around that size too).

It's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs - I've seen "Agentic Engineering" show up in a few other places recently. most notable [from Andrej Karpathy](https://twitter.com/karpathy/status/2019137879310836075) and [Addy Osmani](https://addyosmani.com/blog/agentic-engineering/).

I ran my "Generate an SVG of a pelican riding a bicycle" prompt through GLM-5 via [OpenRouter](https://openrouter.ai/) and got back [a very good pelican on a disappointing bicycle frame](https://gist.github.com/simonw/cc4ca7815ae82562e89a9fdd99f0725d):

![The pelican is good and has a well defined beak. The bicycle frame is a wonky red triangle. Nice sun and motion lines.](https://static.simonwillison.net/static/2026/glm-5-pelican.png)

---

## Key Takeaways

### Notable Quotes

> This is a huge new MIT-licensed model: 754B parameters and 1.51TB on Hugging Face

*Context: Discussing the size and significance of the GLM-5 model.*

> It's interesting to see Z.ai take a position on what we should call professional software engineers building with LLMs

*Context: Commenting on the emerging term 'Agentic Engineering' for software development with LLMs.*

> I ran my 'Generate an SVG of a pelican riding a bicycle' prompt through GLM-5

*Context: Illustrating the practical use of GLM-5 with a creative example.*

## Related Topics

- [[topics/agent-native-architecture]]
- [[topics/prompt-engineering]]
- [[topics/ai-agents]]

---

## Related Articles

<CardGroup cols={1}>
  <Card
    title="[AINews] Z.ai GLM-5: New SOTA Open Weights LLM"
    icon="newspaper"
    href="/kb/articles/ainews-zai-glm-5-new-sota-open-weights-llm-be70bd66"
  >
    Swyx · reference · 73% similar
  </Card>
  <Card
    title="ggml.ai joins Hugging Face to ensure the long-term progress of Local AI"
    icon="newspaper"
    href="/kb/articles/ggmlai-joins-hugging-face-to-ensure-the-long-term-progress-o-915b03d0"
  >
    Simon Willison · explanation · 71% similar
  </Card>
  <Card
    title="Captaining IMO Gold, Deep Think, On-Policy RL, Feeling the AGI in Singapore — Yi Tay"
    icon="newspaper"
    href="/kb/articles/captaining-imo-gold-deep-think-on-policy-rl-feeling-the-agi--815e6177"
  >
    Swyx · explanation · 71% similar
  </Card>
</CardGroup>

---

<Note>
Originally published at [https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything](https://simonwillison.net/2026/Feb/11/glm-5/#atom-everything).
</Note>
